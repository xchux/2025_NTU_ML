{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1TFwaJir_Olj"
   },
   "source": [
    "# ML2025 Homework 1 - Retrieval Augmented Generation with Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6tQHdH2k_Olk"
   },
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-_ZkNxqGGhdl"
   },
   "source": [
    "First, we will mount your own Google Drive and change the working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17912,
     "status": "ok",
     "timestamp": 1747788882260,
     "user": {
      "displayName": "Simon Chu",
      "userId": "02458227940771957056"
     },
     "user_tz": -480
    },
    "id": "DWQh-lq8GuwZ",
    "outputId": "70bf7816-b5a1-478c-9a2c-d8b171324342"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive/\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 177,
     "status": "ok",
     "timestamp": 1747788883348,
     "user": {
      "displayName": "Simon Chu",
      "userId": "02458227940771957056"
     },
     "user_tz": -480
    },
    "id": "P_5Tf1rMHBQ-",
    "outputId": "09447284-0e43-4899-ce36-414109d3fc8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/2025_NTU_ML\n"
     ]
    }
   ],
   "source": [
    "# Change the working directory to somewhere in your Google Drive.\n",
    "# You could check the path by right clicking on the folder.\n",
    "%cd /content/drive/MyDrive/2025_NTU_ML/HW1/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mGx000oZ_Oll"
   },
   "source": [
    "In this section, we install the necessary python packages and download model weights of the quantized version of LLaMA 3.1 8B. Also, download the dataset. Note that the model weight is around 8GB. If you are using your Google Drive as the working directory, make sure you have enough space for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6535,
     "status": "ok",
     "timestamp": 1747789102487,
     "user": {
      "displayName": "Simon Chu",
      "userId": "02458227940771957056"
     },
     "user_tz": -480
    },
    "id": "5JywoPOO_Oll",
    "outputId": "2ff5d128-bfae-4bfc-c10d-fbc23b22337d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://abetlen.github.io/llama-cpp-python/whl/cu122\n",
      "Requirement already satisfied: llama-cpp-python==0.3.4 in /usr/local/lib/python3.11/dist-packages (0.3.4)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python==0.3.4) (4.13.2)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python==0.3.4) (2.0.2)\n",
      "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python==0.3.4) (5.6.3)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python==0.3.4) (3.1.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.11.3->llama-cpp-python==0.3.4) (3.0.2)\n",
      "Requirement already satisfied: googlesearch-python in /usr/local/lib/python3.11/dist-packages (1.3.0)\n",
      "Requirement already satisfied: bs4 in /usr/local/lib/python3.11/dist-packages (0.0.2)\n",
      "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (3.4.2)\n",
      "Requirement already satisfied: requests-html in /usr/local/lib/python3.11/dist-packages (0.10.0)\n",
      "Requirement already satisfied: lxml_html_clean in /usr/local/lib/python3.11/dist-packages (0.4.2)\n",
      "Requirement already satisfied: beautifulsoup4>=4.9 in /usr/local/lib/python3.11/dist-packages (from googlesearch-python) (4.13.4)\n",
      "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.11/dist-packages (from googlesearch-python) (2.32.3)\n",
      "Requirement already satisfied: pyquery in /usr/local/lib/python3.11/dist-packages (from requests-html) (2.0.1)\n",
      "Requirement already satisfied: fake-useragent in /usr/local/lib/python3.11/dist-packages (from requests-html) (2.2.0)\n",
      "Requirement already satisfied: parse in /usr/local/lib/python3.11/dist-packages (from requests-html) (1.20.2)\n",
      "Requirement already satisfied: w3lib in /usr/local/lib/python3.11/dist-packages (from requests-html) (2.3.1)\n",
      "Requirement already satisfied: pyppeteer>=0.0.14 in /usr/local/lib/python3.11/dist-packages (from requests-html) (2.0.0)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from lxml_html_clean) (5.4.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.9->googlesearch-python) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.9->googlesearch-python) (4.13.2)\n",
      "Requirement already satisfied: appdirs<2.0.0,>=1.4.3 in /usr/local/lib/python3.11/dist-packages (from pyppeteer>=0.0.14->requests-html) (1.4.4)\n",
      "Requirement already satisfied: certifi>=2023 in /usr/local/lib/python3.11/dist-packages (from pyppeteer>=0.0.14->requests-html) (2025.4.26)\n",
      "Requirement already satisfied: importlib-metadata>=1.4 in /usr/local/lib/python3.11/dist-packages (from pyppeteer>=0.0.14->requests-html) (8.7.0)\n",
      "Requirement already satisfied: pyee<12.0.0,>=11.0.0 in /usr/local/lib/python3.11/dist-packages (from pyppeteer>=0.0.14->requests-html) (11.1.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from pyppeteer>=0.0.14->requests-html) (4.67.1)\n",
      "Requirement already satisfied: urllib3<2.0.0,>=1.25.8 in /usr/local/lib/python3.11/dist-packages (from pyppeteer>=0.0.14->requests-html) (1.26.20)\n",
      "Requirement already satisfied: websockets<11.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from pyppeteer>=0.0.14->requests-html) (10.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->googlesearch-python) (3.10)\n",
      "Requirement already satisfied: cssselect>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from pyquery->requests-html) (1.3.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=1.4->pyppeteer>=0.0.14->requests-html) (3.21.0)\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install --no-cache-dir llama-cpp-python==0.3.4 --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu122\n",
    "!python3 -m pip install googlesearch-python bs4 charset-normalizer requests-html lxml_html_clean\n",
    "\n",
    "from pathlib import Path\n",
    "if not Path('./Meta-Llama-3.1-8B-Instruct-Q8_0.gguf').exists():\n",
    "    !wget https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\n",
    "if not Path('./public.txt').exists():\n",
    "    !wget https://www.csie.ntu.edu.tw/~ulin/public.txt\n",
    "if not Path('./private.txt').exists():\n",
    "    !wget https://www.csie.ntu.edu.tw/~ulin/private.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "error",
     "timestamp": 1747789130720,
     "user": {
      "displayName": "Simon Chu",
      "userId": "02458227940771957056"
     },
     "user_tz": -480
    },
    "id": "kX6SizAt_Olm",
    "outputId": "618a6e00-6101-4b8c-b38c-e559b2676f44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are good to go!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if not torch.cuda.is_available():\n",
    "    raise Exception('You are not using the GPU runtime. Change it first or you will suffer from the super slow inference speed!')\n",
    "else:\n",
    "    print('You are good to go!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l3iyc1qC_Olm"
   },
   "source": [
    "## Prepare the LLM and LLM utility function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T59vxAo2_Olm"
   },
   "source": [
    "By default, we will use the quantized version of LLaMA 3.1 8B. you can get full marks on this homework by using the provided LLM and LLM utility function. You can also try out different LLM models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vtepTeT3_Olm"
   },
   "source": [
    "In the following code block, we will load the downloaded LLM model weights onto the GPU first.\n",
    "Then, we implemented the generate_response() function so that you can get the generated response from the LLM model more easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eVil2Vhe_Olm"
   },
   "source": [
    "You can ignore \"llama_new_context_with_model: n_ctx_per_seq (16384) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\" warning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "ScyW45N__Olm",
    "outputId": "f86466a1-36ad-4c53-d387-00077b9ae77d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_new_context_with_model: n_ctx_per_seq (16384) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "# Load the model onto GPU\n",
    "llama3 = Llama(\n",
    "    \"./Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\",\n",
    "    verbose=False,\n",
    "    n_gpu_layers=-1,\n",
    "    n_ctx=16384,    # This argument is how many tokens the model can take. The longer the better, but it will consume more memory. 16384 is a proper value for a GPU with 16GB VRAM.\n",
    ")\n",
    "\n",
    "def generate_response(_model: Llama, _messages: str) -> str:\n",
    "    '''\n",
    "    This function will inference the model with given messages.\n",
    "    '''\n",
    "    _output = _model.create_chat_completion(\n",
    "        _messages,\n",
    "        stop=[\"<|eot_id|>\", \"<|end_of_text|>\"],\n",
    "        max_tokens=512,    # This argument is how many tokens the model can generate, you can change it and observe the differences.\n",
    "        temperature=0,      # This argument is the randomness of the model. 0 means no randomness. You will get the same result with the same input every time. You can try to set it to different values.\n",
    "        repeat_penalty=2.0,\n",
    "    )[\"choices\"][0][\"message\"][\"content\"]\n",
    "    return _output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tnHLwq-4_Olm"
   },
   "source": [
    "## Search Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SYM-2ZsE_Olm"
   },
   "source": [
    "The TA has implemented a search tool for you to search certain keywords using Google Search. You can use this tool to search for the relevant **web pages** for the given question. The search tool can be integrated in the following sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "bEIRmZl7_Oln",
    "outputId": "4ff06005-b579-4a75-b714-fa154ba51dd1"
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from googlesearch import search as _search\n",
    "from bs4 import BeautifulSoup\n",
    "from charset_normalizer import from_bytes\n",
    "import asyncio\n",
    "from requests_html import AsyncHTMLSession\n",
    "import urllib3\n",
    "urllib3.disable_warnings()\n",
    "MAX_TOKENS = 15000\n",
    "\n",
    "async def worker(s:AsyncHTMLSession, url:str):\n",
    "    try:\n",
    "        header_response = await asyncio.wait_for(s.head(url, verify=False), timeout=10)\n",
    "        if 'text/html' not in header_response.headers.get('Content-Type', ''):\n",
    "            return None\n",
    "        r = await asyncio.wait_for(s.get(url, verify=False), timeout=10)\n",
    "        return r.text\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "async def get_htmls(urls):\n",
    "    session = AsyncHTMLSession()\n",
    "    tasks = (worker(session, url) for url in urls)\n",
    "    return await asyncio.gather(*tasks)\n",
    "\n",
    "async def search(keyword: str, n_results: int=3, _model: Llama = llama3) -> List[str]:\n",
    "    '''\n",
    "    This function will search the keyword and return the text content in the first n_results web pages.\n",
    "\n",
    "    Warning: You may suffer from HTTP 429 errors if you search too many times in a period of time. This is unavoidable and you should take your own risk if you want to try search more results at once.\n",
    "    The rate limit is not explicitly announced by Google, hence there's not much we can do except for changing the IP or wait until Google unban you (we don't know how long the penalty will last either).\n",
    "    '''\n",
    "    filtered_results = []\n",
    "    keyword = keyword[:100]\n",
    "    # First, search the keyword and get the results. Also, get 2 times more results in case some of them are invalid.\n",
    "    results = list(_search(keyword, n_results * 2, lang=\"zh\", unique=True))\n",
    "    # Filter out PDF and other non-HTML URLs\n",
    "    results = [url for url in results if not url.lower().endswith('.pdf')]\n",
    "    # Then, get the HTML from the results. Also, the helper function will filter out the non-HTML urls.\n",
    "    htmls = await get_htmls(results)\n",
    "    for html in htmls:\n",
    "        # Filter out the None values.\n",
    "        if html is None:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Step 1: str 轉回 bytes\n",
    "            if isinstance(html, bytes):\n",
    "                html_bytes = html\n",
    "            elif isinstance(html, str):\n",
    "                try:\n",
    "                    html_bytes = html.encode('latin1')  # fallback 保留原始 bytes\n",
    "                except UnicodeEncodeError:\n",
    "                    html_bytes = html.encode('utf-8')  # 優先當作 utf-8\n",
    "\n",
    "            # Step 2: 編碼偵測與轉換\n",
    "            result = from_bytes(html_bytes).best()\n",
    "            if result is None:\n",
    "                continue\n",
    "            html_decoded = str(result)\n",
    "\n",
    "            # Step 3: 拿乾淨文字\n",
    "            soup = BeautifulSoup(html_decoded, 'html.parser')\n",
    "            filtered_results.append(''.join(soup.get_text().split()))\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    limit_results = []\n",
    "    total_tokens = 0\n",
    "\n",
    "    for result in filtered_results:\n",
    "        encoded = _model.tokenize(result.encode(\"utf-8\"), add_bos=False)\n",
    "        if total_tokens + len(encoded) > MAX_TOKENS or total_tokens > MAX_TOKENS:\n",
    "            continue\n",
    "        total_tokens += len(encoded)\n",
    "        limit_results.append(result)\n",
    "\n",
    "    # if not limit_results:\n",
    "    #     # For debugging purpose, you can uncomment the following line to raise an exception if no valid results are found.\n",
    "    #     raise Exception('No valid results found. Please try again later.')\n",
    "    # Return the first n results.\n",
    "    return limit_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rC3zQjjj_Oln"
   },
   "source": [
    "## Test the LLM inference pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9172,
     "status": "ok",
     "timestamp": 1747756670334,
     "user": {
      "displayName": "Simon Chu",
      "userId": "02458227940771957056"
     },
     "user_tz": -480
    },
    "id": "8dmGCARd_Oln",
    "outputId": "2baf0d98-d629-4140-fcc7-b0fd076aba2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "泰勒絲（Taylor Swift）是一位美國歌手、詞曲作家和製作人。她出生於1989年，來自田納西州。她的音樂風格從鄉村搖滾開始逐漸轉變為流行電音。\n",
      "\n",
      "她早期的作品如《泰勒絲第一輯》、《愛情故事第二章：睡美人的秘密》，獲得了廣泛認可和獎項，包括多個告示牌音樂大奖。後來，她推出了更具商業成功性的專辑，如 《1989》（2014）、_reputation（《名聲_(泰勒絲专輯)》） （ 20 ） 和 _Lover(2020)，並且在全球取得了巨大的影響力。\n",
      "\n",
      "她以她的歌曲如 \"Shake It Off\"、\"_Blank Space_\"和 \"_Bad Blood_\",以及與其他藝人合作的作品，如 《Look What You Made Me Do》（2017）而聞名。泰勒絲還是知識產權運動的一部分，對於音樂創作者在數字時代獲得公平報酬有所關注。\n",
      "\n",
      "她被譽為當代最成功和影響力最大的人物之一，並且她的歌曲經常成為流行文化的話題。\n"
     ]
    }
   ],
   "source": [
    "# You can try out different questions here.\n",
    "test_question='請問誰是 Taylor Swift？'\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"你是 LLaMA-3.1-8B，是用來回答問題的 AI。使用中文時只會使用繁體中文來回問題。\"},    # System prompt\n",
    "    {\"role\": \"user\", \"content\": test_question}, # User prompt\n",
    "]\n",
    "\n",
    "print(generate_response(llama3, messages))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C0-ojJuE_Oln"
   },
   "source": [
    "## Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HGsIPud3_Oln"
   },
   "source": [
    "The TA has implemented the Agent class for you. You can use this class to create agents that can interact with the LLM model. The Agent class has the following attributes and methods:\n",
    "- Attributes:\n",
    "    - role_description: The role of the agent. For example, if you want this agent to be a history expert, you can set the role_description to \"You are a history expert. You will only answer questions based on what really happened in the past. Do not generate any answer if you don't have reliable sources.\".\n",
    "    - task_description: The task of the agent. For example, if you want this agent to answer questions only in yes/no, you can set the task_description to \"Please answer the following question in yes/no. Explanations are not needed.\"\n",
    "    - llm: Just an indicator of the LLM model used by the agent.\n",
    "- Method:\n",
    "    - inference: This method takes a message as input and returns the generated response from the LLM model. The message will first be formatted into proper input for the LLM model. (This is where you can set some global instructions like \"Please speak in a polite manner\" or \"Please provide a detailed explanation\".) The generated response will be returned as the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "zjG-UwDX_Oln"
   },
   "outputs": [],
   "source": [
    "class LLMAgent():\n",
    "    def __init__(self, role_description: str, task_description: str, llm:str=\"bartowski/Meta-Llama-3.1-8B-Instruct-GGUF\"):\n",
    "        self.role_description = role_description   # Role means who this agent should act like. e.g. the history expert, the manager......\n",
    "        self.task_description = task_description    # Task description instructs what task should this agent solve.\n",
    "        self.llm = llm  # LLM indicates which LLM backend this agent is using.\n",
    "    def inference(self, message:str) -> str:\n",
    "        if self.llm == 'bartowski/Meta-Llama-3.1-8B-Instruct-GGUF': # If using the default one.\n",
    "            # TODO: Design the system prompt and user prompt here.\n",
    "            # Format the messsages first.\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": f\"{self.role_description}\"},  # Hint: you may want the agents to speak Traditional Chinese only.\n",
    "                {\"role\": \"user\", \"content\": f\"{self.task_description}\\n{message}\"}, # Hint: you may want the agents to clearly distinguish the task descriptions and the user messages. A proper seperation text rather than a simple line break is recommended.\n",
    "            ]\n",
    "            return generate_response(llama3, messages)\n",
    "        else:\n",
    "            # TODO: If you want to use LLMs other than the given one, please implement the inference part on your own.\n",
    "            return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0-ueJrgP_Oln"
   },
   "source": [
    "TODO: Design the role description and task description for each agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "DzPzmNnj_Oln"
   },
   "outputs": [],
   "source": [
    "# TODO: Design the role and task description for each agent.\n",
    "\n",
    "# This agent may help you filter out the irrelevant parts in question descriptions.\n",
    "question_extraction_agent = LLMAgent(\n",
    "    role_description=\"你是 LLaMA-3.1-8B，專門從文本中提取核心問題的 AI。你不提供解答，只專注於問題提取，使用中文時只會使用繁體中文來回問題。\",\n",
    "    task_description=\"請從以下訊息中提取一個完整且精確的問題句。請保留專有名詞（如歌名、人名、地名、活動名稱等），確保語句結構清楚明確，不可將陳述句誤當成問題，也不要曲解原始的問題，且簡單扼要。\"\n",
    ")\n",
    "\n",
    "# This agent may help you extract the keywords in a question so that the search tool can find more accurate results.\n",
    "keyword_extraction_agent = LLMAgent(\n",
    "    role_description=\"你是 LLaMA-3.1-8B，專門從問題中提取關鍵字的 AI。你僅專注於關鍵字提取，不進行問題解答或額外解釋。使用中文時只會使用繁體中文來回問題。\",\n",
    "    task_description=\"請從以下問題中選出你會輸入搜尋引擎的關鍵字。保留所有專有名詞（如歌名、人名、地名、活動名稱、引號內的內容等）以及相關的詞語。僅輸出這些關鍵字，不重述問題，也不得自行生成未在輸入中出現的詞彙。\"\n",
    ")\n",
    "\n",
    "# This agent is the core component that answers the question.\n",
    "qa_agent = LLMAgent(\n",
    "    role_description=\"你是 LLaMA-3.1-8B，是用來回答問題的 AI。使用中文時只會使用繁體中文來回問題。\",\n",
    "    task_description=\"請回答以下問題：\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A9eoywr7_Oln"
   },
   "source": [
    "## RAG pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8HDOjNYJ_Oln"
   },
   "source": [
    "TODO: Implement the RAG pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MRGNa-1i_Oln"
   },
   "source": [
    "Please refer to the homework description slides for hints.\n",
    "\n",
    "Also, there might be more heuristics (e.g. classifying the questions based on their lengths, determining if the question need a search or not, reconfirm the answer before returning it to the user......) that are not shown in the flow charts. You can use your creativity to come up with a better solution!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cMaIsKAZ_Olo"
   },
   "source": [
    "- Naive approach (simple baseline)\n",
    "\n",
    "    ![](https://www.csie.ntu.edu.tw/~ulin/naive.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mppO-oOO_Olo"
   },
   "source": [
    "- Naive RAG approach (medium baseline)\n",
    "\n",
    "    ![](https://www.csie.ntu.edu.tw/~ulin/naive_rag.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HYxbciLO_Olo"
   },
   "source": [
    "- RAG with agents (strong baseline)\n",
    "\n",
    "    ![](https://www.csie.ntu.edu.tw/~ulin/rag_agent.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "ztJkA7R7_Olo"
   },
   "outputs": [],
   "source": [
    "async def pipeline(question: str) -> str:\n",
    "    # TODO: Implement your pipeline.\n",
    "    # Currently, it only feeds the question directly to the LLM.\n",
    "    # You may want to get the final results through multiple inferences.\n",
    "    # Just a quick reminder, make sure your input length is within the limit of the model context window (16384 tokens), you may want to truncate some excessive texts.\n",
    "    keywords = keyword_extraction_agent.inference(question)\n",
    "    # print(f\"Keywords is: {keywords}\")\n",
    "    results = await search(keywords)\n",
    "    # print(f\"Search results are: {results}\")\n",
    "    core_question = question_extraction_agent.inference(question)\n",
    "    # print(\"Core problem is: \", core_question)\n",
    "    return qa_agent.inference(f\"googlesearch: {results}，core question: {core_question}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P_kI_9EGB0S9"
   },
   "source": [
    "## Answer the questions using your pipeline!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PN17sSZ8DUg7"
   },
   "source": [
    "Since Colab has usage limit, you might encounter the disconnections. The following code will save your answer for each question. If you have mounted your Google Drive as instructed, you can just rerun the whole notebook to continue your process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "plUDRTi_B39S",
    "outputId": "e82a68de-d4d4-485d-f576-938de3c94d6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 「虎山雄風飛揚」是光華國小的校歌。\n",
      "2 根據NCC的說明，自2025年初起，如果民眾通過境外郵購無線鍑盤、滑鼠或藍芽耳機等第二級電信管制射頻器材回台，每案都會加收新臺幣750元審查費。\n",
      "3 第一代 iPhone 是由史蒂夫·喬布斯發表的。它於 2007 年六月十九日在美國威利士堡（Walt Disney Concert Hall）舉行的一場盛大新聞會上正式亮相，標誌著手機產業進入智慧型多媒體時代。  第一代 iPhone 的設計由喬布斯親自操刀，他希望創造一個既美觀又易用的手持裝置。這款電話採取了全觸控式操作系統，並且配備了一個 3.5 英寸的彩色液晶顯示屏幕，內建有 Wi-Fi 和 EDGE 網絡連接功能。  第一代 iPhone 的發表引起全球媒體關注，被視為手機產業的一大革命。它不僅改變了人們對於電話和互聯網使用方式，也推動了一系列的創新產品研製，包括後來出現的手寫識別、GPS 和其他多種功能。  喬布斯在發表第一代 iPhone 時曾說道：「我們要做的是將電腦帶到手中，使它們更易於用，並且讓人感到愉快。」這句話成為了他對產品設計和創新的核心理念。\n",
      "4 根據提供的資訊，托福網路測驗 TOEFL iBT 達到 92 分以上才能申請台灣大學進階英文免修。\n",
      "5 在橄欖球運動中，達陣（Try）是一種得分方式。當一名選手將足球觸地於對方的得到區內時，就會獲得5個點數。  根據規則，如果一個隊伍成功完成了一次Touchdown，他們可以選擇進行Kickoff或Dropkick來進攻，而不是直接射門。如果他們決定通過Kicking方式取得分，球員將站在觸地處的正中央，並且必須踢出足球，使其越過對方防守線。\n",
      "6 根據卑南族的神話傳說，人類始祖是從大地中出生的女 thần奴努勞（Nunur），她把一 根竹子插在起源之處巴拿 巴那樣 （Panapanayan） ，而一個男孩和一个 女生分别从 竹子的不同部分 出来。\n",
      "7 熊仔的碩班指導教授為李琳山。\n",
      "8 法拉第是發現電磁感應定律並奠基於其上的人。\n",
      "9 根據提供的資訊，距離國立臺灣史前文化博物館最近的是康樂站。\n",
      "10 根據提供的資訊，三十幾（30几）不包括 3O，而是指大於 Thirty 的數字卻小于 Forty。因此，如果我們將 Twenty 加上 Three 十五，我们可以得到以下結果：  31 +21 =52 32+22=54... 39.8.+29.=68  所以答案應該是在 Fifty 至 Sixty 之間的任意整数。  但如果您問的是 20加30等於多少？那就很簡單了，正確回答是50。\n",
      "11 達拉斯獨行俠隊的Luka Doncic被交易至洛杉磯湖人。\n",
      "12 根據目前的選情，川普和賀錦麗仍在競爭中。然而，如果我們假設以現在的情況為基礎進行預測，那麼以下是可能發生的結果：  1.  川 普：如果共和黨繼續保持領先地位，並且能夠贏得足够的選舉人票，川普很有機會成為下一任美國總統。 2\\. 賀錦麗: 如果民主党能够收拾残局并取得勝利，那麼賀锦丽就可能成为美国总统。  然而，這些都是假設性的預測，並且仍在競爭中。最終結果將取決於選民的投票和各州的情況變化。如果您想了解更多關于2024年美國總統大选的情况，建議你繼續追蹤最新消息並參考可靠資訊來源。  另外，我們需要注意的是，這個問題中提到的川普2.0時代的預測也是基於目前的情況進行的一些分析和推論。然而，在政治競選過程中的變化是無法完全控制或準確预测的事情，最終結果將取決于各方因素。  因此，我們需要保持謹慎並繼續關注最新消息，以便更好地了解2024年美國總統大选的發展情況。\n",
      "13 參數量最小的 Llama-3.2 系列模型是 1B 個参数。\n",
      "14 根據國立臺灣大學學則，停修課程每個月以不超過3分為限。但情況特殊經導師、就讀系（所）主管同意者，不在此限制。\n",
      "15 根據提供的資訊，DeepSeek公司是杭州深度求索人工智能基础技术研究有限公司（Hangzhou DeepSeeK Artificial Intelligence Co., Ltd.）的一部分。\n",
      "16 2024年NBA的總冠軍隊伍是波士頓塞爾提克。\n",
      "17 炔烴\n",
      "18 阿倫·圖靈（AlanTuring）\n",
      "19 根據提供的資訊，玄天上帝信仰進香中心位於南投縣名間鄉松山村。\n",
      "20 是的，Windows 作業系統是一款由微軟公司開發和推出的操作系统。它最初於1985年11月20日被公佈，並且自那時起就一直在發展中。在過去幾十年的時間裡，它已經演變成了一個非常強大的作為，提供了許多功能，如文件管理、程式執行環境等。  Windows 作業系統的主要目的是讓使用者能夠輕鬆地操作電腦，並且可以在上面運用各種軟體和應用的程序。它也支援了一系列語言，使得全球不同地區的人都有可能利用這個作為進行工作、娛樂等活動。  微软Windows11是最新的版本，提供了許多新的功能，如更好的安全性保護措施，以及對於AI技術的一些整合，以便讓使用者能夠更加方安地操作電腦。\n",
      "21 官將首的起源地點是臺灣新北市 新莊地區。\n",
      "22 大黑佛母\n",
      "23 根據提供的歌詞和資訊，這首曲子是由動力火車（PowerStation）演唱，名為「路人甲」。\n",
      "24 2025 年卑南族聯合年聚在利嘉部落舉辦。\n",
      "25 是的，最新出現的是「GeForce RTX 40」系列顯卡。\n",
      "26 根據文章的描述，大S是在日本旅遊時因流感併發肺炎去世。\n",
      "27 艾薩克‧牛頓發現了萬有引力。\n",
      "28 根據提供的資訊，台鵠開示計畫「TAIHUCAIS」的英文全名為：  \"Taiwan Humanities Conversational AI Knowledge Discovery System\"  也就是 \"臺灣人文知識探勘系統生成式對話平台”。\n",
      "29 「I'll be back」是出自1984年阿诺·施瓦辛格主演的科幻电影《终结者》的经典台词。\n",
      "30 水的化學式是H2O。\n",
      "31 根據提供的資訊，李宏毅教授在2023年春季班開設了《機器學習》課程，但沒有提到第15個作業名稱。然而，我們可以看到他另外一門名為「Introduction to Generative AI 」 的 課綱中，有一個與你所問的類似的問題。  根據提供給你的資訊，李宏毅教授在2024年春季班開設了《生成式人工智慧導論》課程，這是一門新創立於該學期。這個新的教科書內容中，並沒有提到任何與你所問的題目相關。  但是在提供給你的資訊，李宏毅教授在2024年春季班開設了《生成式人工智慧導論》課程，這是一門新創立於該學期。這個新的教科書內容中，有一個與你所問的類似的問題。  根據提供給你的資訊，李宏毅教授在2024年春季班開設了《生成式人工智慧導論》課程，這是一門新創立於該學期。這個新的教科書內容中，有一個與你所問的類似的問題。  根據提供給你的資訊，李宏毅教授在2024年春季班開設了《生成式人工智慧導論》課程，這是一門新創立於該學期。這個新的教科書內容中，有一個與你所問的類似的問題。  根據提供給你的資訊，李宏毅教授在2024年春季班開設了《生成式人工智慧導論》課程，這是一門新創立於該學期。這個新的教科書內容中，有一個與你所問的類似的問題。  根據提供給你的資訊，李宏毅教授在2024年春季班開設了《生成式人工智慧導論》課程，這是一門新創立於該學期。這個新的教科書內容中，有一個與你所問的類似的問題。  根據提供給你的資訊，李宏毅教授在2024年春季班開設了《生成式人工智慧導論》課程，這\n",
      "32 根據提供的資訊，公立獨學院僅剩一間，即國防醫學大學。\n",
      "33 在 BitTorrent 協議中，當一個新的節點加入網路時，它如何能夠從其他種子隨機地獲得部分資料？這個問題的答案是通過 DHT（分布式哈希表）技術。  DTH 是一套用於實現去中心化查找和存儲資源的一組算法。它允許節點在網路中找到並訪問其他種子的檔案，無需依賴中央的 Tracker 伺服器或 DNS 查詢服務。  當一個新的 節点加入 BitTorrent 網络时，它会首先與 DHT 网絡中的某些已知节点建立连接。然后，這個新節點會向這些建立連接的大量種子發送查找請求，要求獲得特定檔案的部分資料。  DTH 中使用的一套算法叫做 Kademlia，它是一种分布式哈希表（distributed hash table）的實現。Kadmelaia 算是用來在 DHT 網絡中找到節點和資源，並且它能夠有效地處理網路中的查找請求。  通過這種方式，新的 節点就可以從其他种子隨機獲得部分資料，這樣做的好处是在於減少了對中央 Tracker 伺服器或 DNS 查詢服務 的依賴，使得 BitTorrent 網絡更加去中心化和可靠。\n",
      "34 根據提取的問題句 \"那個影片阿？\"，我推測這是因為網頁提示使用支援瀏覽器（Chrome、Firefox或Edge）來觀看某段視頻內容。\n",
      "35 答案：戈芬氏鳳頭鸚鵡喜歡的乳酪口味是「藍莓豆漿」\n",
      "36 根據文章的描述，最後這隻企鵝寶嬰兒名子將在12月10日公布。\n",
      "37 根據提供的資訊，國立臺灣大學物理治療學系正常修業年限為六個月。\n",
      "38 根據萌娘百科的資料，Pastel*Palettes的一員角色是Rico Sasaki，她笑聲習慣就是「呼嘿 嘻」！\n",
      "39 日本戰國時代被稱為「甲斐之虎」的武田信玄是誰？\n",
      "40 根據王肥貓同學的標準，他最有可能去修「國民法官必備之基礎鑑識科학」課程。\n",
      "41 2024年12月25日\n",
      "42 馬智禮是出身於福建的閩南人，後來成為初鹿頭目的漢裔。\n",
      "43 《BanG Dream! Ave Mujica》的片頭曲是 《黑のバースデイ》。\n",
      "44 Linux作業系統最早於1991年10月5日首次發布。\n",
      "45 利嘉部落的中文名稱是「Likavung」，而在台東縣卑南鄉也有其他稱呼，如呂家望、吕佳等。\n",
      "46 紅茶是全發酵的。\n",
      "47 《遊戲王》中的融合怪獸是指通過將兩個或以上的卡片組成一個新的、更強大的生物來創造的一種特殊類型。這些牌通常需要特定的素材和條件才能召喚，然後就能發揮出其驚人的力量。  在《遊戲王》中，有許多不同的融合怪獸，每一隻都有自己的獨立的能力、攻擊力等屬性。在本文提到的超魔導龍騎士-真紅眼黑龙（LGB1-JP001）就是其中的一種，需要以「黒・マジシャン」和 「true red eyes black dragon 」或是其他某些特定卡片作為素材才能召喚。\n",
      "48 豐田萌繪在《BanG Dream!》企劃中，擔任松原花音的聲優。\n",
      "49 在橄欖球運動中，9 號的正式名稱是「傳鋒」（Scrum-half）。\n",
      "50 答案是冥王星。\n",
      "51 根據提供的資訊，臺灣最早成立野生動物救傷單位是農業部生物多樣性研究所所屬之「國立特有植物研究保育中心」、「南投縣自然保護區管理處」，但這些資料並未提及該組織位於哪個行政地理位置。\n",
      "52 根據維基百科的資料，特生中心在2023年改制後更名為「農業部生物多樣性研究所」。\n",
      "53 根據提取的問題句和提供給我的文獻資料，我們可以看到論 文標題是 \"DeSTA2: Developing Instruction-Following Speech Language Model WithoutSpeechInstruction-Tuning Data\"。  在這篇文章中，作者提出了一個名為 De_STA_  的模型，這是一種能夠跟隨使用者指示的語言模式。\n",
      "54 太陽系中體積最大的行星是木衛三。\n",
      "55 達悟族的原住民族語言與其他南島系臺灣十六個法定認定的原始民間在分類學上一般不被視為同群。\n",
      "56 根據提供的資料，課程名稱為「【 程式設計 】」這個老師有幾位：  1. 曹承礎 2..洪瑞文\n",
      "57 根據提供的資訊，「embiyax namu kana」 是布農族語言中的打招呼用语。\n",
      "58 很抱歉，我們無法在提供的文本中找到關於「鄒與布農，永久美麗」這個歌名是哪位藝人的作品。\n",
      "59 動畫「雖然是公會的櫃檯小姐，但因為不想加班所以打算獨自討伐迷宮頭目」中的女主角亞莉納隱藏了冒險者身份，特別是在她工作時，她是一名普通的地面職員，但是當有需要她的技能和力量來解決問題的場合下，就會變成一位強大的「處刑人」。\n",
      "60 姊妹 Tuku 創建了射馬幹部落。\n",
      "61 KO.1田弘光，战斗指数12000。\n",
      "62 完全公平排程器（Completely Fair Scheduler，CFS）是 Linux 内核的一部分，它負責行 程 排 丁。 C FS 參考了澳大利亞麻醉師康恩·科里瓦斯提出的樓梯調度算法和 RSDL 的經驗，並選取花費 CPU 執 行 時間最少的程式來進行排定。  CFS 主要由 `sched_entity` 內含的一個虛擬執行時間（virtual running time）所決定，不再跟蹤程序睡眠時長，且揚棄了活躍/過期概念。 C FS 使用紅黑樹演算法將 執 行 時間越少的工作排列在左邊，而使用 `dequeue_entity()` 和 重新安插節點（即 rb_node）的方法來完成。  CFS 在 Linux 核心2.6 .23 之後採用，取代了先前的 O(1) 排程器，並成為系統預設的排序機制。它負責將 CPU 資源分配給正在執行中的程序，以最大化互動效能和整體CPU使用率。  CFS 的爭議在於其放棄 RSDL 引起科里瓦斯不滿，導致他一度宣布脫離 Linux 開發團隊。後來，他重新開發展殘排程器對決 C FS，但最終被證明 BFS 並沒有優勢，而是CFS的睡眠公平性在某些情況下會出現調度延遲。  因此，知乎上關於完全 公 平 排 程 器 的 問題 可 能 是：  *   Linux 核心中 C FS 如何使用紅黑樹儘存排程相關資訊？     *  答：CFS 使用了 Red-Black Tree 儅放置行程序的虛擬執 行 時間。      <!---->  *\\*  Linux核心設計:Scheduler-HackMD  O(1) Scheduler 概述CFSScheduler深入剖析 CF Schedulercfs scheduler/PELTc fs  s cheduler /2cpu排程器測試工具sched_extEnergyAwareSchedulingDeadline schedulingPREEMPT_RT{\"title\":\"Linux核心設計:Scheduler\",\"description\":\"\",\"contributors\":[]}Expandmenu', '完全公平排序機制\n",
      "63 諾曼第登陸（Normandy Landings）是第二次世界大戰中盟軍對德國的重大攻勢，於1944年6月發動。這場行動被稱為「奧運會」（Operation Overlord），目的是突破納粹佔領下的法蘭西北部地區，並進一步推向歐洲內陸。  諾曼第登陆是盟軍反擊德國的關鍵步驟，標誌著二戰中的一個重要轉折點。這場行動涉及到大量兵力和物資投入，其中包括空降、海上突襲等多種作戦方式。在此次攻勢下，大量士官與普通軍人犧牲，但盟國最終取得了勝利，為進一步推翻德意志第三帝 chế打開了一道大門。  諾曼第登陸的成功對二戰後期歐洲局面的發展產生深遠影響。\n",
      "64 《Cytus II》遊戲中「Body Talk」是PAFF的歌曲。\n",
      "65 根據提供的資訊，李琳山教授在國立臺灣大學所開設信號與系統課程，在期末考前後會有一次演講，這個題目被稱為「《》」。\n",
      "66 NVIDIA GeForce RTX 5090 顯示卡的 VRAM (視覺記憶體) 是32GB。\n",
      "67 根據提供的資訊，2024年世界棒球12強賽冠軍是中華臺北隊。\n",
      "68 中國四大奇書是指《水滸傳》、《三國演義》， 《金瓶梅》（或稱為「六大小說」時會加上紅樓夢和儒林外史）、以及西遊記。\n",
      "69 根據提取的問題句，子 時是從晚上 11 點（23:00）到隔日凌晨1點 （01：００），也就是說它涵蓋了兩個時間段。\n",
      "70 硬實時間系統（Hard Real-time System）是一種特殊的作業系统，旨在確保所有任務都能夠按時完成，而不會因為其他程序或事件而延遲。這類型的心理需求是避免錯過重要截止期限來執行某些工作。  硬實時間系統通常需要具備以下特性：  1.  **即刻響應**：任務必須能夠在最短的時間內完成。 2\\.   \\*\\*確保安全\\_\\_，例如防火牆、監控等功能。\\*  3.\\&quot;硬實時間系統&quot;\\#&#x20;  4.  **可靠性**：任務必須能夠在任何情況下都完成。  5\\.   \\*\\*穩定\\_\\_，例如不會因為其他程序而延遲。\\*  6.\\&quot;硬實時間系統&quot;\\#&#x20;  7.  **可預測性**：任務的執行結果可以被準確地計算和控制。  8\\.   \\*\\*穩定\\_\\_，例如不會因為其他程序而延遲。\\*  9.\\&quot;硬實時間系統&quot;\\#&#x20;  10. **可維護性**：任務的執行結果可以被輕易地檢查和修正。  11\\.   \\*\\*穩定\\_\\_，例如不會因為其他程序而延遲。\\*  12.\\&quot;硬實時間系統&quot;\\#&#x20;  13. **可擴展性**：任務的執行結果可以被輕易地增加或減少。  14\\.   \\*\\*穩定\\_\\_，例如不會因為其他程序而延遲。\\*  15.\\&quot;硬實時間系統&quot;\\#&#x20;  16. **可升級性**：任務的執行結果可以被輕易地更新和改善。  17\\.   \\*\\*穩定\\_\\_，例如不會因為其他程序而延遲。\\*  18.\\&quot;硬實時間系統&quot;\\#&#x20;  19. **可監控性**：任務的執行結果可以被輕易地檢查和控制。  在軟\n",
      "71 根據原作中的描述，代號「C8763」對應的是桐谷和人持有的劍技——星光連流擊（Starburst Stream）。\n",
      "72 根據文中提到的地名介紹，「柴城」位於現今的屏東縣車埕鄉。\n",
      "73 根據提供的資訊，若要使用A100高級GPU，在Google Colab訂閱制中需要購買Colaboratory Pro+計畫。\n",
      "74 台湾大学\n",
      "75 根據清華大學的規定，學生如果修滿一定數目的分，可以免簽減少課程申請書。雪江同樣可以參考以下資訊：   雪 江 同 學 需 要 修 滿 60 分 才 可 免 簭 減 少 課 程 適 用 規 定：   1. 本校學生修滿一定數目的分，得免簽減少課程申請書。 2..本規定適用於在籍之碳、博班及專科部全日制同等級以上的畢業前最後一屆入学者。  3。依據教育局頒布「大學學士生修滿一定數目的分免簽減少課程申請書規定」，本校自107年9月1 日起實施，適用於在籍之碳、博班及專科部全日制同等級以上的畢業前最後一屆入学者。  4. 本學生修滿60分後，可免簽減少課程申請書，但仍須繼續遵守本校教務章則之規定。\n",
      "76 Neuro-sama 的最初 Live2D 模型是使用 VTube Studio \"桃瀨日和／ 桃瀬ひより\" 作為形象（V1）。\n",
      "77 從零開始的異世界生活 第三季中，劫持愛蜜莉雅並想取其為妻的人是雷格魯斯・柯尼亞士。\n",
      "78 根據你提供的資訊，我找到了答案。  在海綿寶宝第五季《失蹤記》中，主角們前往了阿拉斯加州（Alaska）擊敗刺破泡沫紅眼幇。\n",
      "79 玉米是一種單子葉植物，而不是雙子的。它的特徵是具有1枚小型、扁平且無柄的小叶，花瓣數為3或其倍数，並有散生的維管束組織。  在生物學中，一般將被動物分成兩大類：單子葉植物和双生子的。雙子の則包括菊科（Asteraceae）、茜草目等許多種屬的花卉，例如玫瑰、牽牛菜及百合。  玉米是一個典型代表著被動物的一年生的禾本類作植物，它具有以下特徵：  1.  **單子葉**：每一枚小叶都只有一个，小而扁平，没有柄。 2\\. 花瓣數為3或其倍数，通常是三角形的花朵。  因此，我們可以確定玉米是一種典型代表著被動物的一年生的禾本類作植物。\n",
      "80 中華民國陸軍的官方樂章是《風雲起，山河動》，但在問題中的歌詞似乎與這首曲子的內容有所不同。根據我的知識，這個版本可能是一種變體或改編版，而不是正統的一致性。  然而，如果要回答你的核心問題，我會說：中華民國陸軍的官方樂章是《風雲起，山河動》\n",
      "81 根據台大電資學院的課程規劃，物理學系是允許只修習一科生物、化工或生理等相關領域的一個選擇。\n",
      "82 根據維基百科的資料，悲湖（Lacus Doloris）是位於月球表面的一個不規則的小型海洋，由凝固玄武岩熔 lava 所構成。它位于“雪陆”區域，並且介於東北方的地摩斯山脈、澄 海和西南的柯瓦列夫 斗娅环形 山之間。  因此，悲湖位於月球背面的地球面對不到地 球的一側，因此是正確答案。\n",
      "83 《C♯小調第14號鋼琴奏鳴曲》的較為人知的別稱是「月光」或 「MoonlightSonata」。\n",
      "84 根據提供的資訊，阿米斯音樂節是由舒美恩（Suming）所創辦。\n",
      "85 根據遊戲「Poppy Playtime - Chapter 4」的內容，黏土人的名字是波比（Bob）。\n",
      "86 賓茂部落（DjumulJ）位於台東縣金峰鄉。\n",
      "87 米開朗基羅《大衛》雕像最初是在佛罗伦萨的圣母百花教堂（Cattedrale di Santa Maria del Fiore）被創作和展出。\n",
      "88 根據文獻記載，除了蔣中正之外，一位曾短暫晉升特級上將的将领是汪精衛。\n",
      "89 2012年第二賽季世界大赛的總冠軍是台北暗殺星（TPA）。\n",
      "90 在日本麻將中，非莊家一開始的手牌有13張。\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Fill in your student ID first.\n",
    "STUDENT_ID = \"simonchu\"\n",
    "\n",
    "STUDENT_ID = STUDENT_ID.lower()\n",
    "with open('./public.txt', 'r', encoding=\"utf-8\") as input_f:\n",
    "    questions = input_f.readlines()\n",
    "    questions = [l.strip().split(',')[0] for l in questions]\n",
    "    # questions = [questions[21]]\n",
    "    for id, question in enumerate(questions, 1):\n",
    "        if Path(f\"./{STUDENT_ID}_{id}.txt\").exists():\n",
    "            continue\n",
    "        answer = await pipeline(question)\n",
    "        answer = answer.replace('\\n',' ')\n",
    "        print(id, answer)\n",
    "        with open(f'./{STUDENT_ID}_{id}.txt', 'w', encoding=\"utf-8\") as output_f:\n",
    "            print(answer, file=output_f)\n",
    "\n",
    "with open('./private.txt', 'r', encoding=\"utf-8\") as input_f:\n",
    "    questions = input_f.readlines()\n",
    "    # questions = [questions[20]]\n",
    "    for id, question in enumerate(questions, 31):\n",
    "        if Path(f\"./{STUDENT_ID}_{id}.txt\").exists():\n",
    "            continue\n",
    "        answer = await pipeline(question)\n",
    "        answer = answer.replace('\\n',' ')\n",
    "        print(id, answer)\n",
    "        with open(f'./{STUDENT_ID}_{id}.txt', 'a', encoding=\"utf-8\") as output_f:\n",
    "            print(answer, file=output_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "GmLO9PlmEBPn",
    "outputId": "b9cf4b01-ef76-464f-fdcb-6a24290c91dd"
   },
   "outputs": [],
   "source": [
    "# Combine the results into one file.\n",
    "with open(f'./{STUDENT_ID}.txt', 'w', encoding=\"utf-8\") as output_f:\n",
    "    for id in range(1,91):\n",
    "        with open(f'./{STUDENT_ID}_{id}.txt', 'r', encoding=\"utf-8\") as input_f:\n",
    "            answer = input_f.readline().strip()\n",
    "            print(answer, file=output_f)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "provenance": [
    {
     "file_id": "1OGEOSy-Acv-EwuRt3uYOvDM6wKBfSElD",
     "timestamp": 1747752439141
    }
   ]
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
